"""
Cognee Service for Crashwise
Provides integrated Cognee functionality for codebase analysis and knowledge graphs
"""
# Copyright (c) 2026 Crashwise
#
# Licensed under the MIT License. See the LICENSE file for details.


import os
import logging
from pathlib import Path
from typing import Dict, List, Any

logger = logging.getLogger(__name__)


class CogneeService:
    """
    Service for managing Cognee integration with Crashwise
    Handles multi-tenant isolation and project-specific knowledge graphs
    """
    
    def __init__(self, config):
        """Initialize with Crashwise config"""
        self.config = config
        self.cognee_config = config.get_cognee_config()
        self.project_context = config.get_project_context()
        self._cognee = None
        self._user = None
        self._initialized = False
    
    async def initialize(self):
        """Initialize Cognee with project-specific configuration"""
        try:
            # Ensure environment variables for Cognee are set before import
            self.config.setup_cognee_environment()
            logger.debug(
                "Cognee environment configured",
                extra={
                    "data": self.cognee_config.get("data_directory"),
                    "system": self.cognee_config.get("system_directory"),
                },
            )

            import cognee
            self._cognee = cognee
            
            # Configure LLM with API key BEFORE any other cognee operations
            provider = os.getenv("LLM_PROVIDER", "openai")
            model = os.getenv("LLM_MODEL") or os.getenv("LITELLM_MODEL", "gpt-4o-mini")
            api_key = os.getenv("COGNEE_API_KEY") or os.getenv("LLM_API_KEY") or os.getenv("OPENAI_API_KEY")
            endpoint = os.getenv("LLM_ENDPOINT")
            api_version = os.getenv("LLM_API_VERSION")
            max_tokens = os.getenv("LLM_MAX_TOKENS")

            if provider.lower() in {"openai", "azure_openai", "custom"} and not api_key:
                raise ValueError(
                    "OpenAI-compatible API key is required for Cognee LLM operations. "
                    "Set OPENAI_API_KEY, LLM_API_KEY, or COGNEE_LLM_API_KEY in your .env"
                )

            # Expose environment variables for downstream libraries
            os.environ["LLM_PROVIDER"] = provider
            os.environ["LITELLM_MODEL"] = model
            os.environ["LLM_MODEL"] = model
            if api_key:
                os.environ["LLM_API_KEY"] = api_key
                # Maintain compatibility with components still expecting OPENAI_API_KEY
                if provider.lower() in {"openai", "azure_openai", "custom"}:
                    os.environ.setdefault("OPENAI_API_KEY", api_key)
            if endpoint:
                os.environ["LLM_ENDPOINT"] = endpoint
                os.environ.setdefault("LLM_API_BASE", endpoint)
                os.environ.setdefault("OPENAI_API_BASE", endpoint)
                os.environ.setdefault("LITELLM_PROXY_API_BASE", endpoint)
            if api_key:
                os.environ.setdefault("LITELLM_PROXY_API_KEY", api_key)
            if api_version:
                os.environ["LLM_API_VERSION"] = api_version
            if max_tokens:
                os.environ["LLM_MAX_TOKENS"] = str(max_tokens)

            # Configure Cognee's runtime using its configuration helpers when available
            embedding_model = os.getenv("LLM_EMBEDDING_MODEL")
            embedding_endpoint = os.getenv("LLM_EMBEDDING_ENDPOINT")
            if embedding_endpoint:
                os.environ.setdefault("LLM_EMBEDDING_API_BASE", embedding_endpoint)

            if hasattr(cognee.config, "set_llm_provider"):
                cognee.config.set_llm_provider(provider)
                if hasattr(cognee.config, "set_llm_model"):
                    cognee.config.set_llm_model(model)
                if api_key and hasattr(cognee.config, "set_llm_api_key"):
                    cognee.config.set_llm_api_key(api_key)
                if endpoint and hasattr(cognee.config, "set_llm_endpoint"):
                    cognee.config.set_llm_endpoint(endpoint)
            if embedding_model and hasattr(cognee.config, "set_llm_embedding_model"):
                cognee.config.set_llm_embedding_model(embedding_model)
            if embedding_endpoint and hasattr(cognee.config, "set_llm_embedding_endpoint"):
                cognee.config.set_llm_embedding_endpoint(embedding_endpoint)
            if api_version and hasattr(cognee.config, "set_llm_api_version"):
                cognee.config.set_llm_api_version(api_version)
            if max_tokens and hasattr(cognee.config, "set_llm_max_tokens"):
                cognee.config.set_llm_max_tokens(int(max_tokens))

            # Configure graph database
            cognee.config.set_graph_db_config({
                "graph_database_provider": self.cognee_config.get("graph_database_provider", "kuzu"),
            })

            # Set data directories
            data_dir = self.cognee_config.get("data_directory")
            system_dir = self.cognee_config.get("system_directory")

            if data_dir:
                logger.debug("Setting cognee data root", extra={"path": data_dir})
                cognee.config.data_root_directory(data_dir)
            if system_dir:
                logger.debug("Setting cognee system root", extra={"path": system_dir})
                cognee.config.system_root_directory(system_dir)

            # Setup multi-tenant user context
            await self._setup_user_context()

            self._initialized = True
            logger.info(f"Cognee initialized for project {self.project_context['project_name']} "
                       f"with Kuzu at {system_dir}")

        except ImportError:
            logger.error("Cognee not installed. Install with: pip install cognee")
            raise
        except Exception as e:
            logger.error(f"Failed to initialize Cognee: {e}")
            raise
    
    async def create_dataset(self):
        """Create dataset for this project if it doesn't exist"""
        if not self._initialized:
            await self.initialize()
        
        try:
            # Dataset creation is handled automatically by Cognee when adding files
            # We just ensure we have the right context set up
            dataset_name = f"{self.project_context['project_name']}_codebase"
            logger.info(f"Dataset {dataset_name} ready for project {self.project_context['project_name']}")
            return dataset_name
        except Exception as e:
            logger.error(f"Failed to create dataset: {e}")
            raise
    
    async def _setup_user_context(self):
        """Setup user context for multi-tenant isolation"""
        try:
            from cognee.modules.users.methods import create_user, get_user
            
            # Always try fallback email first to avoid validation issues
            fallback_email = f"project_{self.project_context['project_id']}@crashwise.example"
            user_tenant = self.project_context['tenant_id']
            
            # Try to get existing fallback user first
            try:
                self._user = await get_user(fallback_email)
                logger.info(f"Using existing user: {fallback_email}")
                return
            except Exception:
                # User doesn't exist, try to create fallback
                pass
            
            # Create fallback user
            try:
                self._user = await create_user(fallback_email, user_tenant)
                logger.info(f"Created fallback user: {fallback_email} for tenant: {user_tenant}")
                return
            except Exception as fallback_error:
                logger.warning(f"Fallback user creation failed: {fallback_error}")
                self._user = None
                return
            
        except Exception as e:
            logger.warning(f"Could not setup multi-tenant user context: {e}")
            logger.info("Proceeding with default context")
            self._user = None
    
    def get_project_dataset_name(self, dataset_suffix: str = "codebase") -> str:
        """Get project-specific dataset name"""
        return f"{self.project_context['project_name']}_{dataset_suffix}"
    
    async def ingest_text(self, content: str, dataset: str = "crashwise") -> bool:
        """Ingest text content into knowledge graph"""
        if not self._initialized:
            await self.initialize()
        
        try:
            await self._cognee.add([content], dataset)
            await self._cognee.cognify([dataset])
            return True
        except Exception as e:
            logger.error(f"Failed to ingest text: {e}")
            return False
    
    async def ingest_files(self, file_paths: List[Path], dataset: str = "crashwise") -> Dict[str, Any]:
        """Ingest multiple files into knowledge graph"""
        if not self._initialized:
            await self.initialize()
        
        results = {
            "success": 0,
            "failed": 0,
            "errors": []
        }
        
        try:
            ingest_paths: List[str] = []
            for file_path in file_paths:
                try:
                    with open(file_path, 'r', encoding='utf-8'):
                        ingest_paths.append(str(file_path))
                    results["success"] += 1
                except (UnicodeDecodeError, PermissionError) as exc:
                    results["failed"] += 1
                    results["errors"].append(f"{file_path}: {exc}")
                    logger.warning("Skipping %s: %s", file_path, exc)

            if ingest_paths:
                await self._cognee.add(ingest_paths, dataset_name=dataset)
                await self._cognee.cognify([dataset])
            
        except Exception as e:
            logger.error(f"Failed to ingest files: {e}")
            results["errors"].append(f"Cognify error: {str(e)}")
        
        return results
    
    async def search_insights(self, query: str, dataset: str = None) -> List[str]:
        """Search for insights in the knowledge graph"""
        if not self._initialized:
            await self.initialize()
        
        try:
            from cognee.modules.search.types import SearchType
            
            kwargs = {
                "query_type": SearchType.INSIGHTS,
                "query_text": query
            }
            
            if dataset:
                kwargs["datasets"] = [dataset]
            
            results = await self._cognee.search(**kwargs)
            return results if isinstance(results, list) else []
            
        except Exception as e:
            logger.error(f"Failed to search insights: {e}")
            return []
    
    async def search_chunks(self, query: str, dataset: str = None) -> List[str]:
        """Search for relevant text chunks"""
        if not self._initialized:
            await self.initialize()
        
        try:
            from cognee.modules.search.types import SearchType
            
            kwargs = {
                "query_type": SearchType.CHUNKS,
                "query_text": query
            }
            
            if dataset:
                kwargs["datasets"] = [dataset]
            
            results = await self._cognee.search(**kwargs)
            return results if isinstance(results, list) else []
            
        except Exception as e:
            logger.error(f"Failed to search chunks: {e}")
            return []
    
    async def search_graph_completion(self, query: str) -> List[str]:
        """Search for graph completion (relationships)"""
        if not self._initialized:
            await self.initialize()
        
        try:
            from cognee.modules.search.types import SearchType
            
            results = await self._cognee.search(
                query_type=SearchType.GRAPH_COMPLETION,
                query_text=query
            )
            return results if isinstance(results, list) else []
            
        except Exception as e:
            logger.error(f"Failed to search graph completion: {e}")
            return []
    
    async def get_status(self) -> Dict[str, Any]:
        """Get service status and statistics"""
        status = {
            "initialized": self._initialized,
            "enabled": self.cognee_config.get("enabled", True),
            "provider": self.cognee_config.get("graph_database_provider", "kuzu"),
            "data_directory": self.cognee_config.get("data_directory"),
            "system_directory": self.cognee_config.get("system_directory"),
        }
        
        if self._initialized:
            try:
                # Check if directories exist and get sizes
                data_dir = Path(status["data_directory"])
                system_dir = Path(status["system_directory"])
                
                status.update({
                    "data_dir_exists": data_dir.exists(),
                    "system_dir_exists": system_dir.exists(),
                    "kuzu_db_exists": (system_dir / "kuzu_db").exists(),
                    "lancedb_exists": (system_dir / "lancedb").exists(),
                })
                
            except Exception as e:
                status["status_error"] = str(e)
        
        return status
    
    async def clear_data(self, confirm: bool = False):
        """Clear all ingested data (dangerous!)"""
        if not confirm:
            raise ValueError("Must confirm data clearing with confirm=True")
        
        if not self._initialized:
            await self.initialize()
        
        try:
            await self._cognee.prune.prune_data()
            await self._cognee.prune.prune_system(metadata=True)
            logger.info("Cognee data cleared")
        except Exception as e:
            logger.error(f"Failed to clear data: {e}")
            raise


class CrashwiseCogneeIntegration:
    """
    Main integration class for Crashwise + Cognee
    Provides high-level operations for security analysis
    """
    
    def __init__(self, config):
        self.service = CogneeService(config)
    
    async def analyze_codebase(self, path: Path, recursive: bool = True) -> Dict[str, Any]:
        """
        Analyze a codebase and extract security-relevant insights
        """
        # Collect code files
        from crashwise_ai.ingest_utils import collect_ingest_files

        files = collect_ingest_files(path, recursive, None, [])
        
        if not files:
            return {"error": "No files found to analyze"}
        
        # Ingest files
        results = await self.service.ingest_files(files, "security_analysis")
        
        if results["success"] == 0:
            return {"error": "Failed to ingest any files", "details": results}
        
        # Extract security insights
        security_queries = [
            "vulnerabilities security risks",
            "authentication authorization",
            "input validation sanitization", 
            "encryption cryptography",
            "error handling exceptions",
            "logging sensitive data"
        ]
        
        insights = {}
        for query in security_queries:
            insight_results = await self.service.search_insights(query, "security_analysis")
            if insight_results:
                insights[query.replace(" ", "_")] = insight_results
        
        return {
            "files_processed": results["success"],
            "files_failed": results["failed"],
            "errors": results["errors"],
            "security_insights": insights
        }
    
    async def query_codebase(self, query: str, search_type: str = "insights") -> List[str]:
        """Query the ingested codebase"""
        if search_type == "insights":
            return await self.service.search_insights(query)
        elif search_type == "chunks":
            return await self.service.search_chunks(query)
        elif search_type == "graph":
            return await self.service.search_graph_completion(query)
        else:
            raise ValueError(f"Unknown search type: {search_type}")
    
    async def get_project_summary(self) -> Dict[str, Any]:
        """Get a summary of the analyzed project"""
        # Search for general project insights
        summary_queries = [
            "project structure components",
            "main functionality features",
            "programming languages frameworks",
            "dependencies libraries"
        ]
        
        summary = {}
        for query in summary_queries:
            results = await self.service.search_insights(query)
            if results:
                summary[query.replace(" ", "_")] = results[:3]  # Top 3 results
        
        return summary
